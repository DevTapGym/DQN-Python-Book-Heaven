# Kiáº¿n trÃºc Há»‡ thá»‘ng DQN Recommendation

## ğŸ“ Tá»•ng quan Kiáº¿n trÃºc

Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn **Deep Q-Network (DQN)** - má»™t thuáº­t toÃ¡n Deep Reinforcement Learning Ä‘á»ƒ gá»£i Ã½ sáº£n pháº©m thÃ´ng minh. Model há»c tá»« feedback cá»§a user (click/purchase) Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng recommendation theo thá»i gian.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND                                 â”‚
â”‚  (User Interface - 3 positions: Home, Search, Cart)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API LAYER (FastAPI)                         â”‚
â”‚  â€¢ POST /recommend    â†’ Gá»£i Ã½ sáº£n pháº©m                          â”‚
â”‚  â€¢ POST /train        â†’ Training tá»« feedback                     â”‚
â”‚  â€¢ GET  /status       â†’ Tráº¡ng thÃ¡i model                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DQN AGENT (Core Logic)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  1. State Encoder: Raw Data â†’ State Vector (87-dim) â”‚       â”‚
â”‚  â”‚  2. DQN Model: Neural Network (3 layers)            â”‚       â”‚
â”‚  â”‚  3. Target Network: Stable Q-value estimation        â”‚       â”‚
â”‚  â”‚  4. Replay Buffer: Experience storage (10K capacity)â”‚       â”‚
â”‚  â”‚  5. Epsilon-Greedy: Exploration vs Exploitation     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MODEL PERSISTENCE                              â”‚
â”‚  â€¢ dqn_model.pt: Main model (auto-save after each train)       â”‚
â”‚  â€¢ checkpoints/: Backup every 100 trains                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ CÃ¡c ThÃ nh Pháº§n ChÃ­nh

### 1. **State Encoder** (`state_encoder.py`)

**Chá»©c nÄƒng:** Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ´ tá»« user thÃ nh vector sá»‘ cá»‘ Ä‘á»‹nh (87 chiá»u) Ä‘á»ƒ Ä‘Æ°a vÃ o neural network.

#### Cáº¥u trÃºc State Vector (87 chiá»u):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMON FEATURES (15 chiá»u)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Gender (3): One-hot [Male, Female, Other]                â”‚
â”‚  â€¢ Age Group (5): One-hot [U20, U30, U40, U50, U60]        â”‚
â”‚  â€¢ Day of Week (7): One-hot [Mon-Sun]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POSITION-SPECIFIC FEATURES (69 chiá»u)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CART:                                                       â”‚
â”‚    â€¢ Num products (1): Normalized [0-20] â†’ [0,1]           â”‚
â”‚    â€¢ Total value (1): Normalized [0-20M] â†’ [0,1]           â”‚
â”‚    â€¢ Avg value (1): Normalized [0-2M] â†’ [0,1]              â”‚
â”‚    â€¢ Product IDs (50): One-hot encoding                     â”‚
â”‚    â€¢ Categories (10): Multi-hot encoding                    â”‚
â”‚    â€¢ Padding (6): Zeros                                     â”‚
â”‚                                                              â”‚
â”‚  SEARCH:                                                     â”‚
â”‚    â€¢ Recent searches (1): Normalized [0-50] â†’ [0,1]        â”‚
â”‚    â€¢ Product IDs (50): One-hot encoding                     â”‚
â”‚    â€¢ Categories (10): Multi-hot encoding                    â”‚
â”‚    â€¢ Padding (8): Zeros                                     â”‚
â”‚                                                              â”‚
â”‚  HOME:                                                       â”‚
â”‚    â€¢ Product IDs (50): One-hot (trending/popular)           â”‚
â”‚    â€¢ Categories (10): Multi-hot (preferences)               â”‚
â”‚    â€¢ Padding (9): Zeros                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POSITION ENCODING (3 chiá»u)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Position (3): One-hot [search, cart, home]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         =
              TOTAL: 87 DIMENSIONS
```

#### Normalization Strategy:

- **Numeric values:** Min-max normalization vá» [0, 1]
- **Categorical:** One-hot hoáº·c Multi-hot encoding
- **Missing values:** Default values (gender="Other", age="U30")

---

### 2. **DQN Model** (`model.py`)

**Kiáº¿n trÃºc Neural Network:**

```
Input Layer (87)
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FC1 (87 â†’ 128)  â”‚  â† Fully Connected Layer 1
â”‚  + ReLU          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FC2 (128 â†’ 128) â”‚  â† Fully Connected Layer 2
â”‚  + ReLU          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FC3 (128 â†’ 50)  â”‚  â† Output Layer (Q-values)
â”‚  (No activation) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
Output: Q-values (50)
[Q(s,1), Q(s,2), ..., Q(s,50)]
```

**Äáº·c Ä‘iá»ƒm:**

- **Architecture:** 3-layer feedforward network
- **Hidden dimensions:** 128 neurons per layer
- **Activation:** ReLU (Rectified Linear Unit)
- **Output:** Raw Q-values (khÃ´ng cÃ³ activation á»Ÿ output)
- **Parameters:** ~20K trainable parameters

---

### 3. **DQN Agent** (`agent.py`)

**TrÃ¡i tim cá»§a há»‡ thá»‘ng** - Quáº£n lÃ½ toÃ n bá»™ logic training vÃ  recommendation.

#### ThÃ nh pháº§n:

```python
class DQNAgent:
    â€¢ model: Main DQN network (online network)
    â€¢ target_model: Target DQN network (stable)
    â€¢ optimizer: Adam optimizer (lr=0.001)
    â€¢ memory: Replay Buffer (capacity=10000)
    â€¢ epsilon: Exploration rate (0.5 â†’ 0.3)
    â€¢ gamma: Discount factor (0.99)
```

#### Chiáº¿n lÆ°á»£c Epsilon-Greedy:

```
Epsilon (Îµ) Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Epoch:    0      500    1000   1500   2000+
Epsilon: 0.5 â†’  0.4  â†’  0.35 â†’ 0.32 â†’ 0.30
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Strategy:
  â€¢ Îµ% â†’ Random action (Exploration)
  â€¢ (1-Îµ)% â†’ Best Q-value action (Exploitation)

Final: 30% exploration, 70% exploitation
```

**LÃ½ do epsilon_min = 0.3:**

- LuÃ´n khÃ¡m phÃ¡ 30% sáº£n pháº©m má»›i
- CÃ¢n báº±ng giá»¯a diversity vÃ  relevance
- PhÃ¹ há»£p vá»›i recommendation system (khÃ´ng cáº§n optimal policy tuyá»‡t Ä‘á»‘i)

---

### 4. **Replay Buffer** (`replay_buffer.py`)

**Chá»©c nÄƒng:** LÆ°u trá»¯ experiences Ä‘á»ƒ training off-policy.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replay Buffer (FIFO Queue)                     â”‚
â”‚  Capacity: 10,000 experiences                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Each Experience:                               â”‚
â”‚    (state, action, reward, next_state, done)   â”‚
â”‚                                                  â”‚
â”‚  Example:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ state: [0.5, 0, 1, ..., 0.8]  (87-dim)â”‚    â”‚
â”‚  â”‚ action: 15  (Product ID)               â”‚    â”‚
â”‚  â”‚ reward: 1.0  (Purchase)                â”‚    â”‚
â”‚  â”‚ next_state: [0.5, 0, 1, ..., 0.9]      â”‚    â”‚
â”‚  â”‚ done: False                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Operations:
  â€¢ push(): Add new experience (auto-remove oldest if full)
  â€¢ sample(batch_size): Random sample for training
  â€¢ __len__(): Current buffer size
```

**Lá»£i Ã­ch:**

- **Break correlation:** Training trÃªn batch random â†’ stable learning
- **Data efficiency:** Reuse experiences multiple times
- **Online learning:** LiÃªn tá»¥c update tá»« user feedback

---

## ğŸ”„ Flow Hoáº¡t Äá»™ng Chi Tiáº¿t

### Flow 1: **Recommendation Flow** (GET suggestions)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER      â”‚
â”‚  on Cart    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. Request recommendation
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend gá»­i raw_data + position        â”‚
â”‚  {                                       â”‚
â”‚    "raw_data": {                         â”‚
â”‚      "gender": "Female",                 â”‚
â”‚      "age_group": "U30",                 â”‚
â”‚      "num_products": 3,                  â”‚
â”‚      "product_ids": [1, 5, 10], ...     â”‚
â”‚    },                                    â”‚
â”‚    "position": "cart"                    â”‚
â”‚  }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. POST /recommend
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Layer (FastAPI)                     â”‚
â”‚  â€¢ Validate input data                   â”‚
â”‚  â€¢ Call state_encoder                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 3. encode_state()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  State Encoder                           â”‚
â”‚  â€¢ Normalize numeric values              â”‚
â”‚  â€¢ One-hot encode categoricals           â”‚
â”‚  â€¢ Concatenate to 87-dim vector          â”‚
â”‚                                           â”‚
â”‚  Output: state = [0.5, 0, 1, ..., 0.8]  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 4. select_top_actions(state, k=10)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DQN Agent - Decision Making             â”‚
â”‚                                           â”‚
â”‚  IF model NOT trained:                   â”‚
â”‚    â†’ Random select 10 products           â”‚
â”‚  ELSE:                                    â”‚
â”‚    Random number r âˆˆ [0,1]               â”‚
â”‚    IF r < epsilon:                       â”‚
â”‚      â†’ Random select 10 (Exploration)    â”‚
â”‚    ELSE:                                  â”‚
â”‚      â†’ DQN forward pass                  â”‚
â”‚      â†’ Select top 10 Q-values            â”‚
â”‚        (Exploitation)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 5. Forward Pass (if exploitation)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DQN Model                               â”‚
â”‚  state (87) â†’ FC1 â†’ ReLU â†’ FC2 â†’ ReLU   â”‚
â”‚            â†’ FC3 â†’ Q-values (50)         â”‚
â”‚                                           â”‚
â”‚  Output: [2.3, 1.5, ..., 3.8, ...]      â”‚
â”‚           â†‘high Q-value = good product   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 6. Top-K Selection
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sort Q-values descending                â”‚
â”‚  Pick top 10 indices                     â”‚
â”‚  Convert: index â†’ Product ID (+1)        â”‚
â”‚                                           â”‚
â”‚  Example: [15, 23, 8, 42, 19, ...]      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 7. Return response
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Response                            â”‚
â”‚  {                                       â”‚
â”‚    "recommended_products": [15,23,8,...],â”‚
â”‚    "count": 10,                          â”‚
â”‚    "strategy": "epsilon-greedy (Îµ=0.35)",â”‚
â”‚    "model_status": "trained"             â”‚
â”‚  }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 8. Display recommendations
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER sees   â”‚
â”‚  Top 10      â”‚
â”‚  Products    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Flow 2: **Training Flow** (Learn from feedback)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER      â”‚
â”‚  clicks or  â”‚
â”‚  purchases  â”‚
â”‚  Product 15 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. User interaction event
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend tracks interaction             â”‚
â”‚  â€¢ Capture: current state, action, resultâ”‚
â”‚  â€¢ Calculate reward:                     â”‚
â”‚    - Click: 0.5                          â”‚
â”‚    - Purchase: 1.0                       â”‚
â”‚  â€¢ Capture next state                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. POST /train
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Layer                               â”‚
â”‚  Receives:                               â”‚
â”‚  {                                       â”‚
â”‚    "raw_data": {...},  // before         â”‚
â”‚    "position": "cart",                   â”‚
â”‚    "action": 15,       // clicked productâ”‚
â”‚    "reward": 1.0,      // purchased!     â”‚
â”‚    "next_raw_data": {...},  // after     â”‚
â”‚    "next_position": "cart",              â”‚
â”‚    "done": false                         â”‚
â”‚  }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 3. Encode states
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  State Encoder (called 2x)               â”‚
â”‚  â€¢ state = encode(raw_data, position)    â”‚
â”‚  â€¢ next_state = encode(next_raw_data, ...)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 4. Store experience
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replay Buffer.push()                    â”‚
â”‚  â€¢ Add (s, a, r, s', done) to buffer     â”‚
â”‚  â€¢ Current size: 1251 / 10000            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 5. Sample batch for training
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replay Buffer.sample(batch_size=32)     â”‚
â”‚  â€¢ Random sample 32 experiences          â”‚
â”‚  â€¢ Convert to numpy arrays               â”‚
â”‚  â€¢ Convert to PyTorch tensors            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 6. Calculate target Q-values
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Target Network Forward Pass             â”‚
â”‚  FOR each next_state in batch:           â”‚
â”‚    Q_target(next_state) â†’ [50 Q-values]  â”‚
â”‚    max_Q = max(Q_target(next_state))     â”‚
â”‚                                           â”‚
â”‚  Bellman Equation:                       â”‚
â”‚  Q_target_value = r + Î³ * max_Q * (1-done)â”‚
â”‚                                           â”‚
â”‚  where:                                  â”‚
â”‚    r = reward (0.5 or 1.0)               â”‚
â”‚    Î³ = 0.99 (discount factor)            â”‚
â”‚    done = False usually                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 7. Calculate current Q-values
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Network Forward Pass               â”‚
â”‚  FOR each state in batch:                â”‚
â”‚    Q_current(state) â†’ [50 Q-values]      â”‚
â”‚    Q_current[action] = predicted Q-value â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 8. Calculate loss
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Loss Function: MSE Loss                 â”‚
â”‚                                           â”‚
â”‚  Loss = Mean(                            â”‚
â”‚    (Q_current[action] - Q_target_value)Â² â”‚
â”‚  )                                        â”‚
â”‚                                           â”‚
â”‚  Goal: Make predicted Q-value close to   â”‚
â”‚        actual reward + future reward     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 9. Backpropagation
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Optimizer (Adam)                        â”‚
â”‚  â€¢ optimizer.zero_grad()                 â”‚
â”‚  â€¢ loss.backward()  â† compute gradients  â”‚
â”‚  â€¢ optimizer.step() â† update weights     â”‚
â”‚                                           â”‚
â”‚  â†’ DQN model weights updated!            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 10. Update epsilon
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epsilon Decay                           â”‚
â”‚  epsilon = max(0.3, epsilon * 0.995)     â”‚
â”‚  â†’ Gradually shift to exploitation       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 11. Save model (EVERY train)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Persistence                       â”‚
â”‚  â€¢ Save to dqn_model.pt                  â”‚
â”‚  â€¢ Include: model, target, optimizer, Îµ  â”‚
â”‚                                           â”‚
â”‚  IF train_count % 100 == 0:              â”‚
â”‚    â€¢ Update target network               â”‚
â”‚    â€¢ Create checkpoint backup            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 12. Return training status
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Response                            â”‚
â”‚  {                                       â”‚
â”‚    "status": "trained",                  â”‚
â”‚    "epsilon": 0.348,                     â”‚
â”‚    "memory_size": 1251,                  â”‚
â”‚    "train_count": 1251,                  â”‚
â”‚    "model_activated": true,              â”‚
â”‚    "model_saved": true                   â”‚
â”‚  }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 13. Log & Continue
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  System      â”‚
â”‚  ready for   â”‚
â”‚  next requestâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Thuáº­t ToÃ¡n DQN Chi Tiáº¿t

### Bellman Equation (Core of Q-Learning):

```
Q(s, a) = r + Î³ * max[Q(s', a')]
          â””â”€â”˜   â””â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚     â”‚         â”‚
           â”‚     â”‚         â””â”€ Best future Q-value
           â”‚     â””â”€ Discount factor (0.99)
           â””â”€ Immediate reward

Where:
  s  = current state
  a  = action taken
  r  = reward received
  s' = next state
  Î³  = discount factor (how much we value future rewards)
```

## ğŸ“Š Training Process (Step by Step)

### Single Training Step:

```python
def train_step(batch_size=32):
    # Step 1: Sample batch from replay buffer
    states, actions, rewards, next_states, dones = memory.sample(32)

    # Step 2: Convert to tensors
    states = torch.FloatTensor(states)          # (32, 87)
    actions = torch.LongTensor(actions)         # (32,)
    rewards = torch.FloatTensor(rewards)        # (32,)
    next_states = torch.FloatTensor(next_states)# (32, 87)
    dones = torch.FloatTensor(dones)            # (32,)

    # Step 3: Calculate target Q-values (no gradient)
    with torch.no_grad():
        q_next = target_model(next_states)      # (32, 50)
        max_q_next = q_next.max(dim=1)[0]       # (32,)
        q_target = rewards + 0.99 * max_q_next * (1 - dones)

    # Step 4: Calculate current Q-values
    q_values = model(states)                    # (32, 50)
    q_current = q_values.gather(1, actions.unsqueeze(1))  # (32, 1)

    # Step 5: Calculate loss
    loss = MSELoss(q_current, q_target)

    # Step 6: Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Step 7: Update epsilon
    epsilon = max(0.3, epsilon * 0.995)
```

### Strategy Evolution:

```
Phase 1: Cold Start (train_count = 0)
â”œâ”€ Strategy: 100% Random
â”œâ”€ Model: Not activated
â””â”€ Goal: Collect initial data

Phase 2: Early Training (train_count = 1-200)
â”œâ”€ Strategy: 50% Random, 50% Model
â”œâ”€ Epsilon: 0.5 â†’ 0.4
â””â”€ Goal: Learn basic patterns

Phase 3: Mid Training (train_count = 200-1000)
â”œâ”€ Strategy: 40% Random, 60% Model
â”œâ”€ Epsilon: 0.4 â†’ 0.32
â””â”€ Goal: Refine recommendations

Phase 4: Mature (train_count > 1000)
â”œâ”€ Strategy: 30% Random, 70% Model
â”œâ”€ Epsilon: 0.3 (stable)
â””â”€ Goal: Balance quality & diversity
```

### Recommendation Quality:

```
Early Stage (train_count < 100):
  â†’ Mostly random, diverse but not relevant

Mid Stage (train_count 100-500):
  â†’ Learning patterns, improving relevance

Mature Stage (train_count > 500):
  â†’ High relevance + 30% diversity
```

## ğŸš€ Deployment Flow

### Complete Lifecycle:

```
1. STARTUP
   â”œâ”€ Initialize API (FastAPI)
   â”œâ”€ Initialize DQN Agent
   â”œâ”€ Check for existing model
   â”‚   â”œâ”€ IF dqn_model.pt exists:
   â”‚   â”‚   â””â”€ Load model â†’ Ready immediately
   â”‚   â””â”€ ELSE:
   â”‚       â””â”€ Cold start â†’ Random recommendations
   â””â”€ API ready to serve

2. RECOMMENDATION PHASE
   â”œâ”€ User visits page (Home/Search/Cart)
   â”œâ”€ Frontend calls POST /recommend
   â”œâ”€ API returns top 10 products
   â””â”€ User sees recommendations

3. INTERACTION PHASE
   â”œâ”€ User clicks/purchases a product
   â”œâ”€ Frontend calls POST /train
   â”œâ”€ Model updates (1 training step)
   â”œâ”€ Model auto-saves
   â””â”€ Next recommendation will be better

4. CONTINUOUS LEARNING
   â”œâ”€ More users â†’ More feedback
   â”œâ”€ More training â†’ Better model
   â”œâ”€ Epsilon decreases â†’ More exploitation
   â””â”€ Quality improves over time

5. RESTART (Safe)
   â”œâ”€ Stop API
   â”œâ”€ Model already saved
   â”œâ”€ Restart API
   â”œâ”€ Auto-load model
   â””â”€ Continue from where it stopped
```

---

## ğŸ“ Æ¯u Äiá»ƒm cá»§a Kiáº¿n TrÃºc

### 1. **Online Learning**

- Há»c liÃªn tá»¥c tá»« user feedback real-time
- KhÃ´ng cáº§n offline training phase
- Model cáº£i thiá»‡n theo thá»i gian

### 2. **Personalized Context**

- State vector chá»©a Ä‘áº§y Ä‘á»§ context (user, position, products)
- 3 vá»‹ trÃ­ khÃ¡c nhau â†’ 3 contexts khÃ¡c nhau
- Model há»c pattern riÃªng cho tá»«ng context

### 3. **Exploration vs Exploitation**

- Epsilon-greedy Ä‘áº£m báº£o diversity
- 30% exploration â†’ KhÃ¡m phÃ¡ sáº£n pháº©m má»›i
- 70% exploitation â†’ Gá»£i Ã½ cháº¥t lÆ°á»£ng cao

### 4. **Stable Training**

- Replay buffer breaks correlation
- Target network prevents moving target
- Gradual epsilon decay ensures convergence

### 5. **Fault Tolerance**

- Auto-save sau má»—i train
- Checkpoint backups
- Restart-safe (load from file)
